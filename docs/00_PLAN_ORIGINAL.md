# üî• Plan d'Impl√©mentation IronForge

> **IronForge** - Un planificateur de t√¢ches distribu√© de haute performance √©crit en Rust.

---

## 1. Objectif & Positionnement March√©

### Le Probl√®me
Les solutions actuelles (Celery, BullMQ, Sidekiq) souffrent de :
| Probl√®me | Impact |
|----------|--------|
| Consommation m√©moire √©lev√©e | Co√ªts cloud x2-x5 |
| D√©pendance au langage (Python/Ruby/Node) | Pas de polyglotte |
| Latence de d√©queue | Retards en cascade |
| Gestion des erreurs basique | Jobs perdus |

### Notre Solution
**IronForge** : Un scheduler **polyglotte** (API HTTP), ultra-rapide (~100k jobs/sec), avec garantie de livraison "at-least-once" et consommation m√©moire < 50MB par worker.

---

## 2. Architecture Syst√®me

```mermaid
flowchart TB
    subgraph Clients["Clients (HTTP)"]
        C1["App Python"]
        C2["App Node.js"]
        C3["Microservice Go"]
    end

    subgraph IronForge["IronForge Server"]
        API["API REST (Axum)"]
        SCHED["Scheduler Core"]
        PRIO["Priority Router"]
    end

    subgraph Storage["Stockage (Redis)"]
        MAIN["Queue Principale (ZSET)"]
        DLQ["Dead Letter Queue (LIST)"]
        META["Job Metadata (HASH)"]
        LOCKS["Verrous Distribu√©s (SET)"]
    end

    subgraph Workers["Workers (Rust SDK)"]
        W1["Worker Pool 1"]
        W2["Worker Pool 2"]
    end

    C1 & C2 & C3 --> API
    API --> SCHED
    SCHED --> PRIO
    PRIO --> MAIN
    MAIN <--> W1 & W2
    W1 & W2 --> DLQ
    META <--> API
    META <--> W1 & W2
    LOCKS <--> W1 & W2
```

---

## 3. Revue Utilisateur Requise

> [!IMPORTANT]
> **Choix Technique #1 : Backend Redis**
> Nous utilisons Redis (Sorted Sets) pour la v1. Avantages : atomicit√©, BLPOP, √©cosyst√®me √©tabli.
> Alternative future : Mode "embedded" avec Sled/Redb pour d√©ploiement sans d√©pendance.

> [!WARNING]
> **D√©cision Requise : Garantie de Livraison**
> - **At-Least-Once** (recommand√©) : Jobs r√©ex√©cut√©s si timeout, n√©cessite idempotence c√¥t√© handler.
> - **At-Most-Once** : Aucun retry, jobs potentiellement perdus.

---

## 4. Mod√®le de Donn√©es

### 4.1 Structure `Job`
```rust
pub struct Job {
    pub id: Uuid,              // Identifiant unique
    pub kind: String,          // Type de job (ex: "email.send")
    pub payload: Value,        // Donn√©es JSON arbitraires
    pub priority: Priority,    // Low, Medium, High, Critical
    pub status: JobStatus,
    pub max_retries: u8,
    pub retry_count: u8,
    pub created_at: DateTime<Utc>,
    pub scheduled_for: Option<DateTime<Utc>>,  // Jobs diff√©r√©s
    pub timeout_ms: u64,       // Timeout d'ex√©cution
    pub metadata: HashMap<String, String>,     // Tags personnalis√©s
}
```

### 4.2 √âtats du Job
```mermaid
stateDiagram-v2
    [*] --> Queued: Soumission
    Queued --> Running: Worker prend le job
    Running --> Completed: Succ√®s
    Running --> Failed: Erreur
    Failed --> Queued: Retry (si retries < max)
    Failed --> DeadLetter: Retry √©puis√©s
    DeadLetter --> Queued: R√©injection manuelle
```

### 4.3 Niveaux de Priorit√©
| Priorit√© | Score Redis | Cas d'Usage |
|----------|-------------|-------------|
| Critical | 0 | Paiements, Alertes s√©curit√© |
| High | 1000 | Notifications temps r√©el |
| Medium | 2000 | Emails transactionnels |
| Low | 3000 | Rapports, Nettoyage |

---

## 5. Flux de Traitement

### 5.1 Soumission de Job
1. Client POST `/jobs` avec payload JSON
2. Validation du sch√©ma + g√©n√©ration UUID
3. Stockage metadata dans `HASH jobs:{id}`
4. Insertion dans `ZSET queue:main` avec score = `priority + timestamp`
5. R√©ponse `202 Accepted` avec `job_id`

### 5.2 Traitement par Worker
1. `BZPOPMIN queue:main` (blocking pop du job prioritaire)
2. Acquisition du verrou distribu√© `SET lock:{job_id} EX 30 NX`
3. Mise √† jour status ‚Üí `Running`
4. Ex√©cution du handler utilisateur
5. **Si succ√®s** : Status ‚Üí `Completed`, lib√©ration verrou
6. **Si √©chec** :
   - `retry_count++`
   - Si `retry_count < max_retries` ‚Üí Requeue avec backoff exponentiel
   - Sinon ‚Üí `LPUSH queue:dlq {job_id}`

### 5.3 Backoff Exponentiel
```
d√©lai = min(base_delay * 2^retry_count, max_delay)
```
| Retry | D√©lai (base=1s, max=5min) |
|-------|---------------------------|
| 1 | 2s |
| 2 | 4s |
| 3 | 8s |
| 4 | 16s |
| 5+ | 5min (plafond) |

---

## 6. API REST

### 6.1 Endpoints
| M√©thode | Endpoint | Description |
|---------|----------|-------------|
| `POST` | `/jobs` | Cr√©er un nouveau job |
| `GET` | `/jobs/:id` | R√©cup√©rer statut et d√©tails |
| `DELETE` | `/jobs/:id` | Annuler un job (si Queued) |
| `POST` | `/jobs/:id/retry` | R√©injecter depuis DLQ |
| `GET` | `/queues/stats` | Statistiques en temps r√©el |
| `GET` | `/health` | Healthcheck |

### 6.2 Exemple de Soumission
```bash
curl -X POST http://localhost:3000/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "kind": "email.send",
    "payload": {
      "to": "user@example.com",
      "template": "welcome"
    },
    "priority": "high",
    "max_retries": 3
  }'
```

**R√©ponse :**
```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "queued",
  "created_at": "2026-01-17T10:30:00Z"
}
```

---

## 7. Observabilit√©

### 7.1 M√©triques Prometheus
```
# Compteurs
ironforge_jobs_submitted_total{priority="high"}
ironforge_jobs_completed_total{kind="email.send"}
ironforge_jobs_failed_total{kind="email.send"}

# Jauges
ironforge_queue_depth{queue="main"}
ironforge_dlq_depth
ironforge_active_workers

# Histogrammes
ironforge_job_duration_seconds{kind="email.send"}
ironforge_job_wait_time_seconds
```

### 7.2 Logging Structur√©
Tous les logs en JSON via `tracing` + `tracing-subscriber` :
```json
{
  "timestamp": "2026-01-17T10:30:00Z",
  "level": "INFO",
  "target": "ironforge::worker",
  "job_id": "550e8400...",
  "kind": "email.send",
  "event": "job.completed",
  "duration_ms": 42
}
```

---

## 8. Structure du Projet

```
iron_forge/
‚îú‚îÄ‚îÄ Cargo.toml
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ lib.rs            # Export public API
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ job.rs        # Struct Job, Priority, Status
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ error.rs      # Types d'erreurs
‚îÇ   ‚îú‚îÄ‚îÄ queue/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis.rs      # Impl√©mentation Redis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ traits.rs     # Trait QueueBackend
‚îÇ   ‚îú‚îÄ‚îÄ worker/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor.rs   # Boucle d'ex√©cution
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handler.rs    # Trait JobHandler
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes.rs     # D√©finition des routes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers.rs   # Logique des endpoints
‚îÇ   ‚îî‚îÄ‚îÄ metrics.rs        # Export Prometheus
‚îú‚îÄ‚îÄ src/bin/
‚îÇ   ‚îî‚îÄ‚îÄ server.rs         # Point d'entr√©e serveur
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ simple_worker.rs  # Worker minimal
‚îÇ   ‚îî‚îÄ‚îÄ submit_jobs.rs    # Client de test
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ integration.rs    # Tests end-to-end
```

---

## 9. D√©pendances Cargo

```toml
[dependencies]
tokio = { version = "1", features = ["full"] }
axum = "0.7"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
redis = { version = "0.25", features = ["tokio-comp", "connection-manager"] }
uuid = { version = "1", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["json"] }
metrics = "0.22"
metrics-exporter-prometheus = "0.14"
thiserror = "1"
```

---

## 10. Plan de V√©rification

### 10.1 Tests Automatis√©s

#### Tests Unitaires
- [ ] S√©rialisation/D√©s√©rialisation Job
- [ ] Calcul du score de priorit√©
- [ ] Logique de backoff exponentiel
- [ ] Validation des payloads

#### Tests d'Int√©gration
```bash
# Pr√©requis : Redis local
docker run -d -p 6379:6379 redis:7-alpine

# Ex√©cuter les tests
cargo test --test integration
```

#### Sc√©narios de Test
1. **Happy Path** : Submit ‚Üí Queue ‚Üí Process ‚Üí Complete
2. **Retry Success** : √âchec puis succ√®s au 2e essai
3. **DLQ Flow** : √âchecs r√©p√©t√©s ‚Üí DLQ ‚Üí R√©injection
4. **Priorit√©** : Critical trait√© avant Low
5. **Timeout** : Job qui d√©passe le timeout ‚Üí Retry

### 10.2 Benchmark de Performance
```bash
# Outil : wrk ou oha
oha -n 100000 -c 100 http://localhost:3000/jobs \
  -m POST \
  -H "Content-Type: application/json" \
  -d '{"kind":"test","payload":{}}'
```

**Objectifs :**
| M√©trique | Cible |
|----------|-------|
| Throughput | > 50,000 jobs/sec |
| Latence P50 | < 1ms |
| Latence P99 | < 10ms |
| M√©moire Worker | < 50MB |

### 10.3 V√©rification Manuelle
1. Lancer le serveur : `cargo run --bin server`
2. Lancer un worker : `cargo run --example simple_worker`
3. Soumettre des jobs via curl
4. Observer les logs et le dashboard Prometheus (`/metrics`)

---

## 11. Roadmap MVP

| Phase | Livrable | Dur√©e Estim√©e |
|-------|----------|---------------|
| **1** | Core : Job, Queue, Redis | 2h |
| **2** | API : POST/GET /jobs | 1h |
| **3** | Worker : Executor, Retry | 2h |
| **4** | Observability : Metrics, Logs | 1h |
| **5** | Tests & Benchmark | 1h |
| **TOTAL** | | **~7h** |

---

## 12. Extensions Futures (Post-MVP)

- [ ] **Cron Jobs** : Planification r√©currente
- [ ] **Workflows (DAG)** : D√©pendances entre jobs
- [ ] **Multi-Tenant** : Isolation par namespace
- [ ] **WebSocket** : Notifications temps r√©el
- [ ] **Dashboard UI** : Interface React/Vue
- [ ] **Mode Embedded** : Sans Redis (Sled)
